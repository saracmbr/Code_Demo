{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYLOQLMXl5nhhZVkz5+73h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saracmbr/Code_Demo/blob/master/classification_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "x8QtrNs266JC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "# The data has already been sorted into training and test sets for us\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0].shape, train_labels[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJQAtAwQ7o7z",
        "outputId": "99f7e6e1-d30c-4b24-df1c-d064e1e297fa"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28, 28), ())"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the min and max values of the training data\n",
        "train_data.min(), train_data.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnt-HOtx9hbE",
        "outputId": "82304edd-3602-4e3c-ce04-7b1e7e6b34f1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=train_data/(train_data.max())\n",
        "\n",
        "test_data=test_data/255."
      ],
      "metadata": {
        "id": "-Sxa18jt99TB"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "6pqg6KeU8H-b"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_data[10], cmap=plt.cm.binary) # change the colours to black & white\n",
        "plt.title(class_names[train_labels[10]]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "v_4K-CVB8UlY",
        "outputId": "5f0ef8a1-2804-4b5e-89c3-0e4079bde7c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
        "  tf.keras.layers.Dense(20, activation=\"relu\"),\n",
        "  ###tf.keras.layers.Dense(4, activation=\"relu\"), was overfitting \n",
        "  tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "]) \n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                 optimizer=tf.keras.optimizers.Adam(),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_data,train_labels,\n",
        "                                epochs=10,\n",
        "                                validation_data=(test_data, test_labels)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mI7BMay9TA6",
        "outputId": "f4033fad-1a4d-4bbf-9c2a-a4e9a9939cff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.5851 - accuracy: 0.7956 - val_loss: 70.3235 - val_accuracy: 0.8120\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4341 - accuracy: 0.8480 - val_loss: 118.5198 - val_accuracy: 0.7828\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4029 - accuracy: 0.8576 - val_loss: 70.0727 - val_accuracy: 0.8194\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3833 - accuracy: 0.8648 - val_loss: 85.2142 - val_accuracy: 0.7988\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3714 - accuracy: 0.8691 - val_loss: 101.2641 - val_accuracy: 0.7904\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3604 - accuracy: 0.8722 - val_loss: 88.1650 - val_accuracy: 0.8019\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3531 - accuracy: 0.8753 - val_loss: 74.6927 - val_accuracy: 0.8280\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3444 - accuracy: 0.8781 - val_loss: 75.2600 - val_accuracy: 0.8201\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3392 - accuracy: 0.8800 - val_loss: 79.3904 - val_accuracy: 0.8213\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3347 - accuracy: 0.8794 - val_loss: 98.0670 - val_accuracy: 0.8038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n",
        "\n",
        "# Fit the model\n",
        "find_lr_history = model.fit(train_data,\n",
        "                               train_labels,\n",
        "                               epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs\n",
        "                               validation_data=(test_data, test_labels),\n",
        "                               callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlWvB_Ecgctr",
        "outputId": "ce8731ff-03ef-458b-b28b-980b74986ed6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3292 - accuracy: 0.8826 - val_loss: 92.1881 - val_accuracy: 0.8057 - lr: 0.0010\n",
            "Epoch 2/40\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3266 - accuracy: 0.8824 - val_loss: 124.5494 - val_accuracy: 0.7866 - lr: 0.0011\n",
            "Epoch 3/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3261 - accuracy: 0.8825 - val_loss: 80.0693 - val_accuracy: 0.8172 - lr: 0.0013\n",
            "Epoch 4/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3255 - accuracy: 0.8829 - val_loss: 101.0263 - val_accuracy: 0.7949 - lr: 0.0014\n",
            "Epoch 5/40\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3271 - accuracy: 0.8813 - val_loss: 109.1458 - val_accuracy: 0.7928 - lr: 0.0016\n",
            "Epoch 6/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3268 - accuracy: 0.8818 - val_loss: 92.9178 - val_accuracy: 0.7998 - lr: 0.0018\n",
            "Epoch 7/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3301 - accuracy: 0.8817 - val_loss: 90.4597 - val_accuracy: 0.8197 - lr: 0.0020\n",
            "Epoch 8/40\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3275 - accuracy: 0.8816 - val_loss: 79.7889 - val_accuracy: 0.8224 - lr: 0.0022\n",
            "Epoch 9/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3309 - accuracy: 0.8809 - val_loss: 85.2385 - val_accuracy: 0.8210 - lr: 0.0025\n",
            "Epoch 10/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3335 - accuracy: 0.8788 - val_loss: 117.3333 - val_accuracy: 0.7891 - lr: 0.0028\n",
            "Epoch 11/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3335 - accuracy: 0.8787 - val_loss: 112.5039 - val_accuracy: 0.7857 - lr: 0.0032\n",
            "Epoch 12/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3375 - accuracy: 0.8780 - val_loss: 93.4178 - val_accuracy: 0.8204 - lr: 0.0035\n",
            "Epoch 13/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3424 - accuracy: 0.8766 - val_loss: 81.7568 - val_accuracy: 0.8242 - lr: 0.0040\n",
            "Epoch 14/40\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3416 - accuracy: 0.8762 - val_loss: 115.4377 - val_accuracy: 0.7882 - lr: 0.0045\n",
            "Epoch 15/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3483 - accuracy: 0.8742 - val_loss: 95.7823 - val_accuracy: 0.7916 - lr: 0.0050\n",
            "Epoch 16/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3524 - accuracy: 0.8726 - val_loss: 117.6199 - val_accuracy: 0.7706 - lr: 0.0056\n",
            "Epoch 17/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3576 - accuracy: 0.8698 - val_loss: 120.2551 - val_accuracy: 0.7971 - lr: 0.0063\n",
            "Epoch 18/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3628 - accuracy: 0.8694 - val_loss: 90.8719 - val_accuracy: 0.8265 - lr: 0.0071\n",
            "Epoch 19/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3708 - accuracy: 0.8650 - val_loss: 116.9859 - val_accuracy: 0.7801 - lr: 0.0079\n",
            "Epoch 20/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3736 - accuracy: 0.8643 - val_loss: 128.2101 - val_accuracy: 0.7772 - lr: 0.0089\n",
            "Epoch 21/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3878 - accuracy: 0.8607 - val_loss: 123.6990 - val_accuracy: 0.7775 - lr: 0.0100\n",
            "Epoch 22/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3990 - accuracy: 0.8569 - val_loss: 97.2239 - val_accuracy: 0.8085 - lr: 0.0112\n",
            "Epoch 23/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4155 - accuracy: 0.8518 - val_loss: 115.9168 - val_accuracy: 0.7746 - lr: 0.0126\n",
            "Epoch 24/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4393 - accuracy: 0.8463 - val_loss: 159.5887 - val_accuracy: 0.7650 - lr: 0.0141\n",
            "Epoch 25/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4515 - accuracy: 0.8416 - val_loss: 96.9352 - val_accuracy: 0.8087 - lr: 0.0158\n",
            "Epoch 26/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4716 - accuracy: 0.8374 - val_loss: 152.5753 - val_accuracy: 0.7407 - lr: 0.0178\n",
            "Epoch 27/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4965 - accuracy: 0.8298 - val_loss: 129.9270 - val_accuracy: 0.7713 - lr: 0.0200\n",
            "Epoch 28/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5325 - accuracy: 0.8207 - val_loss: 162.0080 - val_accuracy: 0.7645 - lr: 0.0224\n",
            "Epoch 29/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5574 - accuracy: 0.8144 - val_loss: 144.7187 - val_accuracy: 0.7489 - lr: 0.0251\n",
            "Epoch 30/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5945 - accuracy: 0.8061 - val_loss: 219.5016 - val_accuracy: 0.7123 - lr: 0.0282\n",
            "Epoch 31/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6500 - accuracy: 0.7935 - val_loss: 155.9555 - val_accuracy: 0.7481 - lr: 0.0316\n",
            "Epoch 32/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.6980 - accuracy: 0.7806 - val_loss: 143.3921 - val_accuracy: 0.7620 - lr: 0.0355\n",
            "Epoch 33/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7456 - accuracy: 0.7591 - val_loss: 146.0000 - val_accuracy: 0.7322 - lr: 0.0398\n",
            "Epoch 34/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8875 - accuracy: 0.7010 - val_loss: 171.6293 - val_accuracy: 0.7119 - lr: 0.0447\n",
            "Epoch 35/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.9385 - accuracy: 0.6846 - val_loss: 58.3176 - val_accuracy: 0.6523 - lr: 0.0501\n",
            "Epoch 36/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0889 - accuracy: 0.6150 - val_loss: 70.2995 - val_accuracy: 0.5823 - lr: 0.0562\n",
            "Epoch 37/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1670 - accuracy: 0.5887 - val_loss: 147.8121 - val_accuracy: 0.5070 - lr: 0.0631\n",
            "Epoch 38/40\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 1.4611 - accuracy: 0.4639 - val_loss: 81.2728 - val_accuracy: 0.4198 - lr: 0.0708\n",
            "Epoch 39/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7303 - accuracy: 0.3731 - val_loss: 51.2231 - val_accuracy: 0.4142 - lr: 0.0794\n",
            "Epoch 40/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6157 - accuracy: 0.3959 - val_loss: 81.1340 - val_accuracy: 0.3521 - lr: 0.0891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
        "  tf.keras.layers.Dense(20, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "]) \n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # different loss function for multiclass classifcation\n",
        "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_data,train_labels,\n",
        "                                epochs=15,\n",
        "                                validation_data=(test_data, test_labels)) # see how the model performs on the test set during training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HjCWWYJlfVu",
        "outputId": "2672a034-955d-471f-8107-82e24c487954"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5919 - accuracy: 0.8004 - val_loss: 0.4844 - val_accuracy: 0.8309\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4369 - accuracy: 0.8479 - val_loss: 0.5183 - val_accuracy: 0.8276\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4056 - accuracy: 0.8578 - val_loss: 0.4361 - val_accuracy: 0.8437\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3837 - accuracy: 0.8659 - val_loss: 0.4165 - val_accuracy: 0.8530\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3709 - accuracy: 0.8690 - val_loss: 0.4367 - val_accuracy: 0.8480\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3593 - accuracy: 0.8732 - val_loss: 0.4191 - val_accuracy: 0.8520\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3513 - accuracy: 0.8755 - val_loss: 0.4155 - val_accuracy: 0.8496\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3418 - accuracy: 0.8792 - val_loss: 0.4009 - val_accuracy: 0.8597\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3357 - accuracy: 0.8799 - val_loss: 0.3914 - val_accuracy: 0.8622\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3315 - accuracy: 0.8817 - val_loss: 0.3919 - val_accuracy: 0.8624\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3253 - accuracy: 0.8820 - val_loss: 0.3909 - val_accuracy: 0.8621\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3205 - accuracy: 0.8842 - val_loss: 0.3947 - val_accuracy: 0.8591\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3166 - accuracy: 0.8859 - val_loss: 0.3829 - val_accuracy: 0.8636\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3123 - accuracy: 0.8873 - val_loss: 0.3802 - val_accuracy: 0.8642\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3097 - accuracy: 0.8878 - val_loss: 0.3882 - val_accuracy: 0.8618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaIbNwQ_riA-",
        "outputId": "4d2ffbcb-95e2-4cf5-f014-2f8cda8f688c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.05098039, 0.2627451 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.19607843, 0.14901961, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.03137255, 0.47058824, 0.81960784,\n",
              "        0.88627451, 0.96862745, 0.92941176, 1.        , 1.        ,\n",
              "        1.        , 0.96862745, 0.93333333, 0.92156863, 0.6745098 ,\n",
              "        0.28235294, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.5372549 , 0.9372549 , 0.98823529, 0.95294118,\n",
              "        0.91764706, 0.89803922, 0.93333333, 0.95686275, 0.96470588,\n",
              "        0.94117647, 0.90196078, 0.90980392, 0.9372549 , 0.97254902,\n",
              "        0.98431373, 0.76078431, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.4       , 1.        , 0.90588235, 0.89411765, 0.89019608,\n",
              "        0.89411765, 0.91372549, 0.90196078, 0.90196078, 0.89803922,\n",
              "        0.89411765, 0.90980392, 0.90980392, 0.90588235, 0.89019608,\n",
              "        0.87843137, 0.98823529, 0.70196078, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.91372549, 0.94509804, 0.89803922, 0.90588235, 1.        ,\n",
              "        1.        , 0.93333333, 0.90588235, 0.89019608, 0.93333333,\n",
              "        0.96470588, 0.89411765, 0.90196078, 0.89019608, 0.91764706,\n",
              "        0.92156863, 0.89803922, 0.94509804, 0.07843137, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.97254902, 0.94509804, 0.90588235, 1.        , 0.58431373,\n",
              "        0.18431373, 0.98823529, 0.89411765, 1.        , 0.94901961,\n",
              "        0.84705882, 0.93333333, 0.90980392, 1.        , 0.89411765,\n",
              "        0.8627451 , 0.91764706, 0.98039216, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        1.        , 0.94117647, 0.90980392, 1.        , 0.05882353,\n",
              "        0.        , 1.        , 0.92941176, 0.74901961, 0.        ,\n",
              "        0.        , 0.83921569, 1.        , 0.05098039, 0.48235294,\n",
              "        1.        , 0.91764706, 0.98823529, 0.44705882, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.02352941,\n",
              "        1.        , 0.93333333, 0.9372549 , 1.        , 0.69411765,\n",
              "        0.        , 1.        , 1.        , 0.        , 0.50980392,\n",
              "        0.45490196, 0.18431373, 0.25490196, 0.16862745, 0.14509804,\n",
              "        1.        , 0.9254902 , 0.97647059, 0.63529412, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.1254902 ,\n",
              "        1.        , 0.9254902 , 0.96078431, 1.        , 0.8       ,\n",
              "        0.        , 1.        , 0.32941176, 0.        , 0.14509804,\n",
              "        0.10980392, 0.12156863, 0.        , 0.09803922, 0.05098039,\n",
              "        1.        , 0.9254902 , 0.97647059, 0.78039216, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.20784314,\n",
              "        1.        , 0.9254902 , 0.98039216, 0.98039216, 0.90588235,\n",
              "        0.00784314, 1.        , 0.08235294, 0.        , 0.86666667,\n",
              "        1.        , 0.9254902 , 0.21176471, 0.96078431, 0.77647059,\n",
              "        0.95294118, 0.93333333, 0.96078431, 0.8745098 , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.31372549,\n",
              "        1.        , 0.92941176, 0.98039216, 0.94117647, 1.        ,\n",
              "        0.        , 0.        , 0.15294118, 0.61568627, 0.        ,\n",
              "        0.        , 0.84313725, 0.36862745, 0.07843137, 0.49411765,\n",
              "        1.        , 0.92941176, 0.9372549 , 0.98039216, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.39607843,\n",
              "        1.        , 0.92156863, 0.99215686, 0.95686275, 0.95294118,\n",
              "        0.52156863, 0.54117647, 0.81568627, 1.        , 0.78823529,\n",
              "        0.83921569, 1.        , 0.90196078, 0.02745098, 0.68235294,\n",
              "        1.        , 0.94117647, 0.93333333, 1.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.49411765,\n",
              "        1.        , 0.91372549, 1.        , 0.97254902, 0.91372549,\n",
              "        1.        , 1.        , 0.94117647, 0.90980392, 0.95294118,\n",
              "        0.95294118, 0.90588235, 0.98431373, 1.        , 1.        ,\n",
              "        0.99607843, 0.95294118, 0.93333333, 1.        , 0.01176471,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.57647059,\n",
              "        1.        , 0.91372549, 0.97647059, 0.70980392, 0.95294118,\n",
              "        0.89019608, 0.87843137, 0.90196078, 0.91764706, 0.90196078,\n",
              "        0.90196078, 0.92156863, 0.89411765, 0.92156863, 0.87058824,\n",
              "        0.81176471, 1.        , 0.9254902 , 1.        , 0.1372549 ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.63921569,\n",
              "        1.        , 0.96078431, 0.86666667, 0.3372549 , 1.        ,\n",
              "        0.91372549, 0.91372549, 0.92156863, 0.9254902 , 0.91764706,\n",
              "        0.91764706, 0.91764706, 0.90980392, 0.94901961, 0.90588235,\n",
              "        0.49019608, 1.        , 0.9254902 , 1.        , 0.21568627,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.70980392,\n",
              "        0.99607843, 1.        , 0.78431373, 0.27058824, 1.        ,\n",
              "        0.89411765, 0.90980392, 0.91764706, 0.92156863, 0.91764706,\n",
              "        0.91764706, 0.91372549, 0.92156863, 0.94509804, 0.92941176,\n",
              "        0.2745098 , 1.        , 0.92156863, 0.96470588, 0.22352941,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.77254902,\n",
              "        0.96862745, 1.        , 0.7372549 , 0.43137255, 1.        ,\n",
              "        0.87843137, 0.91372549, 0.91764706, 0.91764706, 0.91764706,\n",
              "        0.91764706, 0.91764706, 0.91764706, 0.94117647, 0.99215686,\n",
              "        0.27058824, 1.        , 0.9254902 , 0.97254902, 0.30196078,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.78431373,\n",
              "        0.96470588, 1.        , 0.58431373, 0.56862745, 1.        ,\n",
              "        0.8745098 , 0.92156863, 0.91764706, 0.92156863, 0.92156863,\n",
              "        0.92156863, 0.91764706, 0.92941176, 0.91372549, 1.        ,\n",
              "        0.18431373, 1.        , 0.9372549 , 0.97647059, 0.38431373,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.8       ,\n",
              "        0.95294118, 1.        , 0.43529412, 0.67843137, 1.        ,\n",
              "        0.89019608, 0.92156863, 0.92156863, 0.9254902 , 0.92156863,\n",
              "        0.92156863, 0.92156863, 0.9372549 , 0.89803922, 1.        ,\n",
              "        0.0745098 , 0.89019608, 0.96470588, 0.97647059, 0.43137255,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.76862745,\n",
              "        0.94117647, 1.        , 0.42745098, 0.83529412, 0.98039216,\n",
              "        0.89803922, 0.92156863, 0.92156863, 0.9254902 , 0.92156863,\n",
              "        0.92941176, 0.9254902 , 0.92941176, 0.88627451, 1.        ,\n",
              "        0.21568627, 0.79607843, 0.98431373, 0.96078431, 0.47058824,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.75294118,\n",
              "        0.95294118, 1.        , 0.44705882, 0.90980392, 0.94117647,\n",
              "        0.90980392, 0.92156863, 0.92156863, 0.9254902 , 0.91764706,\n",
              "        0.92941176, 0.9254902 , 0.92156863, 0.89803922, 1.        ,\n",
              "        0.5254902 , 0.67058824, 0.98823529, 0.95686275, 0.5372549 ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.74117647,\n",
              "        0.98431373, 1.        , 0.60392157, 0.93333333, 0.91372549,\n",
              "        0.9254902 , 0.91764706, 0.92156863, 0.9254902 , 0.92156863,\n",
              "        0.93333333, 0.9254902 , 0.92156863, 0.90980392, 1.        ,\n",
              "        0.65098039, 0.49019608, 1.        , 0.95294118, 0.55686275,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.71764706,\n",
              "        0.98823529, 1.        , 0.67058824, 0.96862745, 0.90980392,\n",
              "        0.91764706, 0.91764706, 0.91372549, 0.91372549, 0.90980392,\n",
              "        0.91764706, 0.91372549, 0.91764706, 0.91372549, 0.94117647,\n",
              "        0.8745098 , 0.50196078, 1.        , 0.94901961, 0.59215686,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.69803922,\n",
              "        0.95294118, 1.        , 0.22352941, 0.93333333, 0.94509804,\n",
              "        0.93333333, 0.93333333, 0.93333333, 0.92941176, 0.9254902 ,\n",
              "        0.92941176, 0.92941176, 0.94117647, 0.92941176, 0.99607843,\n",
              "        0.69019608, 0.20392157, 1.        , 0.9372549 , 0.61568627,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.7372549 ,\n",
              "        0.94117647, 0.98039216, 0.24313725, 0.85490196, 1.        ,\n",
              "        0.8627451 , 0.87058824, 0.87058824, 0.87058824, 0.8745098 ,\n",
              "        0.8745098 , 0.87843137, 0.87058824, 0.85490196, 1.        ,\n",
              "        0.60392157, 0.1254902 , 1.        , 0.9254902 , 0.7372549 ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.50980392,\n",
              "        0.96078431, 0.94901961, 0.09411765, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.13333333, 0.94901961, 0.95686275, 0.52941176,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.29803922,\n",
              "        1.        , 0.97647059, 0.08627451, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.15294118, 0.97647059, 1.        , 0.48235294,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.19215686,\n",
              "        0.80392157, 0.77254902, 0.04313725, 0.        , 0.01568627,\n",
              "        0.00392157, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
              "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.01176471,\n",
              "        0.        , 0.01176471, 0.68235294, 0.74117647, 0.2627451 ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}